import sys

from utils.rag_fusion import reciprocal_rank_fusion

sys.path.append("./")
from dotenv import load_dotenv
from langchain_google_genai import GoogleGenerativeAIEmbeddings

from router.reflection import ReflectionRouter
from router.semantic_router import TranslationRouter
from utils.chat_history import load_chat_history_from_json

_ = load_dotenv()
from typing import List, Dict, Optional, Union
from pydantic import BaseModel, Field
from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.prompts import ChatPromptTemplate
from prompts import *
from vector_database.index_qdrant import (
    create_collection,
    search_qdrant,
    asearch_qdrant,
)


class Metadata(BaseModel):
    source: str = Field(..., description="Source of the document")
    page: int = Field(..., description="Page number in the document")


class Document(BaseModel):
    metadata: List[Metadata] = Field(
        ...,
        description="The list Metadata related to the source document using in answer.",
    )
    page_content: str = Field(
        ..., description="The content of the answer generated by the LLM."
    )

    class Config:
        json_schema_extra = {"required": ["metadata", "page_content"]}


class Retrieval_LLM:
    def __init__(
        self,
        final_llm: BaseChatModel,
        translation_llm: BaseChatModel,
        reflection_llm: BaseChatModel,
        retrieval,
        sys_prompt: str = "",
        callbacks=None,
    ):
        self.prompt = ChatPromptTemplate.from_template(sys_prompt)
        self.final_llm = self.prompt | final_llm.with_structured_output(Document)
        self.translation_llm = translation_llm
        self.reflection_llm = reflection_llm
        self.retrieval = retrieval
        self.sys_prompt = sys_prompt

    def get_context(self, user_query: str, top_k: int = 5):
        docs = search_qdrant(user_query, self.retrieval, top_k=top_k)
        return docs
        # context = ""
        # for doc in docs:
        #     context = context + doc.page_content
        #     context += f"\nMetadata:\n{doc.metadata}\n"
        # print(context)
        # return context

    def reflect(self, user_query, history_chat):
        # history_chat = load_chat_history_from_json(db_chat, session_id)
        response = self.reflection_llm.rewrite(history_chat, user_query)
        return response.is_rewrite, response.rewrite_mes

    def invoke(self, query, translations_res):
        contexts = []
        for translation in translations_res.queries:
            contexts.append(self.get_context(translation, top_k=5))
        contexts = reciprocal_rank_fusion(contexts)
        str_context = ""
        for i, context in enumerate(contexts[0:5]):
            t = context[0]
            text = t.page_content
            metadata = f'Metadata: source: {t.metadata.get("source", "")}, page: {t.metadata.get("page","")}'
            str_context += f"***Văn bản {i+1}:***\n{text} \n{metadata}\n\n"
        print(str_context)
        response = self.final_llm.invoke(
            {
                "question": query,
                "context": str_context,
            }
        )
        return response


# if __name__ == "__main__":
#     from prompts import *
#     from langchain_groq import ChatGroq
#     llm = ChatGroq(model="llama-3.3-70b-versatile")
#     reflection_llm = ReflectionRouter(llm, sys=rewrite_prompt)
#     translation_llm = TranslationRouter(llm, sys=translation_prompt)
#     embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
#     vector_store = create_collection("langchain", embeddings)
#     retrieval_chat = Retrieval_LLM(
#         final_llm=llm,
#         translation_llm=translation_llm,
#         reflection_llm=reflection_llm,
#         retrieval=vector_store,
#         sys_prompt=human_prompt,
#     )
#     chat_history = load_chat_history_from_json("chat_history.json", "session_1")
#     print(retrieval_chat.invoke("Tôi là ai", chat_history))
#     # chat_history = load_chat_history_from_json("chat_history.json", "session_1")
