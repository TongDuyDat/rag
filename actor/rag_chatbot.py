import sys

sys.path.append("./")
from dotenv import load_dotenv
from langchain_google_genai import GoogleGenerativeAIEmbeddings

from router.reflection import ReflectionRouter
from router.semantic_router import TranslationRouter
from utils.chat_history import load_chat_history_from_json

_ = load_dotenv()
from typing import List, Dict, Optional, Union
from pydantic import BaseModel, Field
from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.prompts import ChatPromptTemplate
from prompts import *
from vector_database.index_qdrant import (
    create_collection,
    search_qdrant,
    asearch_qdrant,
)


class Document(BaseModel):
    metadata: Dict[str, Optional[Union[int, str]]] = Field(
        ..., description="The source documentation related to the answer. The source documentation get on context. If don't answer not answer."
    )
    page_content: str = Field(
        ..., description="The content of the answer generated by the LLM model."
    )
    


class Retrieval_LLM:
    def __init__(
        self,
        final_llm: BaseChatModel,
        translation_llm: BaseChatModel,
        reflection_llm: BaseChatModel,
        retrieval,
        sys_prompt: str = "",
        callbacks=None,
    ):
        self.prompt = ChatPromptTemplate.from_template(sys_prompt)
        self.final_llm = self.prompt | final_llm.with_structured_output(Document)
        self.translation_llm = translation_llm
        self.reflection_llm = reflection_llm
        self.retrieval = retrieval
        self.sys_prompt = sys_prompt

    def get_context(self, user_query: str, top_k: int = 5):
        docs = search_qdrant(user_query, self.retrieval, top_k=top_k)
        context = ""
        for doc in docs:
            context = context + doc.page_content
            context += f"\nMetadata:\n{doc.metadata}\n"
        return context

    def reflect(self, user_query, history_chat):
        # history_chat = load_chat_history_from_json(db_chat, session_id)
        response = self.reflection_llm.rewrite(history_chat, user_query)
        return response.is_rewrite, response.rewrite_mes

    def invoke(self, query, history_chat):
        reflect_query = query
        is_rewrite, context_rewrite = self.reflect(query, history_chat)
        if is_rewrite:
            reflect_query = context_rewrite
        translations_res = self.translation_llm.invoke(reflect_query)
        contexts = ""
        for translation in translations_res.queries:
            contexts = contexts + self.get_context(translation, top_k=2)
        print(contexts)
        response = self.final_llm.invoke(
            {
                "question": reflect_query,
                "context": contexts,
            }
        )
        return response


# if __name__ == "__main__":
#     from prompts import *
#     from langchain_groq import ChatGroq
#     llm = ChatGroq(model="llama-3.3-70b-versatile")
#     reflection_llm = ReflectionRouter(llm, sys=rewrite_prompt)
#     translation_llm = TranslationRouter(llm, sys=translation_prompt)
#     embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
#     vector_store = create_collection("langchain", embeddings)
#     retrieval_chat = Retrieval_LLM(
#         final_llm=llm,
#         translation_llm=translation_llm,
#         reflection_llm=reflection_llm,
#         retrieval=vector_store,
#         sys_prompt=human_prompt,
#     )
#     chat_history = load_chat_history_from_json("chat_history.json", "session_1")
#     print(retrieval_chat.invoke("Tôi là ai", chat_history))
#     # chat_history = load_chat_history_from_json("chat_history.json", "session_1")
